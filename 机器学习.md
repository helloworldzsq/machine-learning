# 机器学习

## 1. 监督学习

监督学习是已经知道数据的label，例如预测房价问题，给出了房子的面积和价格。

- 回归问题是预测连续值的输出，例如预测房价。

  ![image-20200922232432315](https://sanzo.top/img/default/ml/image-20200922232432315.png)

- 分类问题是预测离散值输出，例如判断肿瘤是良性还是恶性。

  ![image-20200922232452094](https://sanzo.top/img/default/ml/image-20200922232452094.png)

## 2. 无监督学习

无监督学习是不知道数据具体的含义，比如给定一些数据但不知道它们具体的信息，对于分类问题无监督学习可以得到多个不同的聚类，从而实现预测的功能。

![image-20200922232542888](https://sanzo.top/img/default/ml/image-20200922232542888.png)

## 3. 线性回归

- 符号定义

![image-20230518111302175](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/image-20230518111302175.png)

- 线性回归是拟合一条线，将训练数据尽可能分布到线上。另外还有多变量的线性回归称为多元线性回归。

![image-20200925221455511](https://sanzo.top/img/default/ml/image-20200925221455511.png)

### 3.1 代价函数

- 如何选择假设函数中的模型参数？

![image-20230518112134071](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/img/image-20230518112134071.png)

- cost function，一般使用最小均方差来评估参数的好坏。

使训练集中预测值和真实值的差的平方的和的2m分之一最小就是所要求的θ0和θ1的值。

![image-20200925222847271](https://sanzo.top/img/default/ml/image-20200925222847271.png)

- 总结

![image-20230518113929292](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/img/image-20230518113929292.png)

- 简化算法，J(θ1)随θ1的变化情况

![image-20200925225215790](https://sanzo.top/img/default/ml/image-20200925225215790.png)

- 三维图

![image-20200925230206340](https://sanzo.top/img/default/ml/image-20200925230206340.png)

- 等高线

![image-20200925230227443](https://sanzo.top/img/default/ml/image-20200925230227443.png)

### 3.2 梯度下降

- 梯度下降，首先为每个参数赋一个初值，通过代价函数的梯度，然后不断地调整参数，最终得到一个局部最优解。初值的不同可能会得到两个不同的结果，即梯度下降不一定得到全局最优解。

![image-20200926193450504](https://sanzo.top/img/default/ml/image-20200926193450504.png)

- 梯度下降在具体的执行时，每一次更新需要同时更新所有的参数。

![image-20200926193952535](https://sanzo.top/img/default/ml/image-20200926193952535.png)

- 梯度下降公式中有两个部分，学习率和偏导数。

偏导数，用来计算当前参数对应代价函数的斜率，导数为正则θ减小，导数为负则θ增大，通过这样的方式可以使整体向θ=0收敛

![image-20200926195243593](https://sanzo.top/img/default/ml/image-20200926195243593.png)

- α用来描述学习率，即每次参数更新的步长。α的大小不好确定，如果太小则需要很多步才能收敛，如果太大最后可能不会收敛甚至可能发散。

![image-20200926195756443](https://sanzo.top/img/default/ml/image-20200926195756443.png)

- 当θ处于局部最优解时，θ的值将不再更新，因为偏导为0。

![image-20200926195859632](https://sanzo.top/img/default/ml/image-20200926195859632.png)

- 这也说明了如果学习率α不改变，参数也可能收敛，假设偏导>0，因为偏导一直在减小，所以每次的步长也会慢慢减小，所以α不需要额外的减小。

![image-20230518123758174](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/img/image-20230518123758174.png)

#### **3.2.1 单元梯度下降**

梯度下降每次更新的都需要进行偏导计算，这个偏导对应线性回归的代价函数。

![image-20200926202751517](https://sanzo.top/img/default/ml/image-20200926202751517.png)

对代价函数求导的结果为：

![image-20200926202845222](https://sanzo.top/img/default/ml/image-20200926202845222.png)

![image-20230518124922431](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/img/image-20230518124922431.png)

梯度下降的过程容易出现局部最优解：

![image-20200926193450504](https://sanzo.top/img/default/ml/image-20200926193450504.png)

但是线性回归的代价函数，往往是一个凸函数。它总能收敛到全局最优。

![image-20200926203154017](https://sanzo.top/img/default/ml/image-20200926203154017.png)

梯度下降过程的动图展示：

![img](https://sanzo.top/img/default/ml/image-20200926203154027.gif)

#### **3.2.2 多元梯度下降**

通常问题都会涉及到多个变量，例如房屋价格预测就包括，面积、房间个数、楼层、价格等。

各个符号的定义如下：

![image-20230518154428474](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/img/image-20230518154428474.png)

因此代价函数就不再只包含一个变量，为了统一可以对常量引入变量 x0=1

![image-20200927122848759](https://sanzo.top/img/default/ml/image-20200927122848759.png)

虽然参数的个数增多，但是对每个参数求偏导时和单个参数类似。

![image-20200927123117176](https://sanzo.top/img/default/ml/image-20200927123117176.png)

#### 3.2.3 特征缩放

多个变量的度量不同，数字之间相差的大小也不同，如果可以将所有的特征变量缩放到大致相同范围，这样会减少梯度算法的迭代。特征缩放不一定非要落到[-1，1]之间，只要数据足够接近就可以。

对训练集中特征 Xi 进行均值归一化：$$Xi = \frac{{Xi - \mu}}{{\sigma}}$$  μ表示平均值，σ表示标准差。

![image-20200927122159371](https://sanzo.top/img/default/ml/image-20200927122159371.png)

![image-20200927122356465](https://sanzo.top/img/default/ml/image-20200927122356465.png)

![image-20200927122645246](https://sanzo.top/img/default/ml/image-20200927122645246.png)

#### 3.2.4 学习率

学习率α的大小会影响梯度算法的执行，太大可能会导致算法不收敛，太小会增加迭代的次数。

可以画出每次迭代的J(θ)的变化，来判断当前算法执行的情况，然后选择合适的学习率。（调参开始…）

![image-20200927124215044](https://sanzo.top/img/default/ml/image-20200927124215044.png)

![image-20200927124314735](https://sanzo.top/img/default/ml/image-20200927124314735.png)

- **Batch梯度下降**：每一步梯度下降，都需要遍历整个训练集样本。
- 可以自由选择特征
- 通过设计不同的特征，能够用更复杂的函数拟合数据，而不是只用一条直线去拟合。

![image-20230518180409702](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/img/image-20230518180409702.png)

## 4. 矩阵和向量

- 一些数学计算转化为矩阵的形式，可以简化代码书写、提高效率、代码更容易理解。

![image-20200926215031531](https://sanzo.top/img/default/ml/image-20200926215031531.png)

![image-20200926223409425](https://sanzo.top/img/default/ml/image-20200926223409425.png)

**矩阵乘法不满足交换律：**

![image-20200926225453052](https://sanzo.top/img/default/ml/image-20200926225453052.png)

**矩阵乘法满足结合律**：

![image-20200926225542850](https://sanzo.top/img/default/ml/image-20200926225542850.png)

**单位矩阵**：

![image-20200926225634974](https://sanzo.top/img/default/ml/image-20200926225634974.png)

**矩阵的逆**：

- 首先是方阵
- 不是所有的矩阵都有逆（行列式不为0）

![image-20200926225331312](https://sanzo.top/img/default/ml/image-20200926225331312.png)

**转置矩阵**：

![image-20200926225407464](https://sanzo.top/img/default/ml/image-20200926225407464.png)

## 5. 正则方程

- 偏导等于0对应线性方程的最小值

![image-20200927173714299](https://sanzo.top/img/default/ml/image-20200927173714299.png)

- 利用线性代数的方法直接求解θ

![image-20200927173747699](https://sanzo.top/img/default/ml/image-20200927173747699.png)

θ的推导可以根据等式Xθ=y得到，（XT）X的目的是将矩阵转化为方阵，因为求矩阵的逆的前提是方阵。

矩阵可能存在 不可逆的情况，这时可是删除一些不必要的特征，或使用正则化。

- 梯度下降和正则方程的优缺点：
  - 正则方程不用特征缩放
  - 特征值超过10000个以后考虑使用梯度下降算法

![image-20200927174320125](https://sanzo.top/img/default/ml/image-20200927174320125.png)

## 6. 逻辑回归

逻辑回归用于解决分类的问题，如果使用线性回归可能会造成很大的误差；假如样本的标签值为0、1，线性回归输出值是连续的，存在>1和小于0的情况，不符合实际。

如果对于一个均匀的数据，使用线性回归，选取0.5作为分界线，可能会得到一个比较准确的模型，但是如果数据不太均匀就会存在很大的误差。

![image-20201002134249446](https://sanzo.top/img/default/ml/image-20201002134249446.png)

![image-20201002134545596](https://sanzo.top/img/default/ml/image-20201002134545596.png)

#### 6.1 激活函数

- sigmoid.py

![image-20201002142636471](https://sanzo.top/img/default/ml/image-20201002142636471.png)

激活函数的y值分布在[0,1]内，对于分类问题，可以使用激活函数的值来表示满足特征的概率。

![image-20201002144146860](https://sanzo.top/img/default/ml/image-20201002144146860.png)

#### 6.2 决策界限

==决策边界是假设函数的一个属性，取决于函数的参数，而不是数据集。==

![image-20230526130946234](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/image-20230526130946234.png)

![image-20201002151014707](https://sanzo.top/img/default/ml/image-20201002151014707.png)

- 粉红色的线条即为决策界限

![image-20201002151544351](https://sanzo.top/img/default/ml/image-20201002151544351.png)

![image-20201002151951691](https://sanzo.top/img/default/ml/image-20201002151951691.png)

#### 6.3 代价函数

对于函数$f(x)=\frac{1}{1+e^{-x}}$，如果使用类似线性回归的代价函数$\Sigma(h(x)-y)^2$，将得到一个非凸函数，这样就不能使用梯度下降的方法求解全局最优解。

![image-20201002155109200](https://sanzo.top/img/default/ml/image-20201002155109200.png)

逻辑回归一般使用**对数函数**作为代价函数：

首先对于分类函数来说，他的输出值范围为[0,1]，得到的对数图像如下：

![image-20201002155225235](https://sanzo.top/img/default/ml/image-20201002155225235.png)

当评估模型参数对y=1（恶性 肿瘤）进行预测的好坏时，如果实际为恶性，预测值也为1（恶性），此时的代价为0；如果实际为恶性，预测为0（良性），此时的代价为$+\infty$，这时代价函数就很好的评估了参数θ的表现。

![image-20201002160155129](https://sanzo.top/img/default/ml/image-20201002160155129.png)

同样对于y=0（良性肿瘤）的代价函数为：

![image-20201002160126721](https://sanzo.top/img/default/ml/image-20201002160126721.png)

y的取值只有0、1，可以将上面两个函数合成一个，评估当前参数的$J(\theta)$为：

![image-20201002193349426](https://sanzo.top/img/default/ml/image-20201002193349426.png)

#### 6.4 梯度下降

在确定代价函数之后的任务是，如何最小化代价函数，因为代价函数是凸的，所以可以使用梯度下降求解。

![image-20201002193549679](https://sanzo.top/img/default/ml/image-20201002193549679.png)

![image-20201002193728758](https://sanzo.top/img/default/ml/image-20201002193728758.png)

==虽然求偏导之后，θ更新的形式和线性回归类似，但是他们本质不同，因为$h_\theta(x)$完全不一样==

#### 6.5 多元分类

![image-20201002200132671](https://sanzo.top/img/default/ml/image-20201002200132671.png)

对每个特征单独训练，在做预测的时候，取三个分类器结果最大的。

![image-20201002200303264](https://sanzo.top/img/default/ml/image-20201002200303264.png)

## 7. 过拟合

存在多个特征，但是数据很少，或者模型函数不合理，都会出现过拟合的现象。过拟合可能对样本数能够很好的解释，但是无法正确的预测新数据。

泛化能力：一个假设模型应用到新模型的能力

![image-20201002211447558](https://sanzo.top/img/default/ml/image-20201002211447558.png)

![image-20201002211511303](https://sanzo.top/img/default/ml/image-20201002211511303.png)

## 8. 正则化

![image-20201002211815208](https://sanzo.top/img/default/ml/image-20201002211815208.png)

解决过拟合的方法：

1. 尽量减少选取变量的数量

2. 正则化处理过拟合问题

![image-20230601153152756](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/image-20230601153152756.png)

参数值值越小，得到的函数就会越平滑，也越简单，因此，也越不容易出现过拟合问题。

在代价函数中加入正则项，通过$$\lambda$$来平衡拟合程度和参数的大小，θ越大越容易出现过拟合的现象。

![image-20201009160444537](https://sanzo.top/img/default/ml/image-20201009160444537.png)

如果$$\lambda$$过大，导致θ≈0，那么最终只剩下下θ0，图像将变成一个直线。

![image-20201009160357606](https://sanzo.top/img/default/ml/image-20201009160357606.png)

### 8.1 线性回归的正则化

- 梯度下降正则化

θ0和其他θ参数分开运算，因为θ0没有进行正则化。

正则化所做的就是每次迭代将θ参数缩小一点，然后进行和之前一样的更新操作

![image-20230601160552865](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/image-20230601160552865.png)

- 正规方程正则化

进行正则化还可以解决X乘以X的转置出现不可逆的问题

![image-20230601160908982](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/image-20230601160908982.png)

### 8.2 逻辑回归的正则化

![image-20201028164800636](https://sanzo.top/img/default/ml/image-20201028164800636.png)

## 9. 神经网络

### 9.1 神经网络简介

大多数的机器学习所涉及到的特征非常多，对于非线性分类问题，往往需要构造多项式来表示数据之间的关系，多项式的组成方式千变万化，这对计算带来一定困扰。

![image-20201015221136066](https://sanzo.top/img/default/ml/image-20201015221136066.png)

大脑中的神经元结构：

![image-20201015221446454](https://sanzo.top/img/default/ml/image-20201015221446454.png)

机器学习中的神经网络一般包括三部分，输入层，隐藏层，输出层。

![image-20201015221559627](https://sanzo.top/img/default/ml/image-20201015221559627.png)

数据从输入层开始，通过激活函数前向传播到第一隐藏层，经过多个隐藏层，最后到达输出层，神经网络表示复杂的逻辑关系，主要是对隐藏层的构造。

![image-20201015221627962](https://sanzo.top/img/default/ml/image-20201015221627962.png)

### 9.2 逻辑运算

XOR 异或运算（相同为0，不同为1）

XNOR 同或运算（相同为1，不同为0）

![image-20201015221807035](https://sanzo.top/img/default/ml/image-20201015221807035.png)

如上为一个XNOR的分类问题，可以搭建出每种逻辑运算的神经网络，最终整合得到XNOR的神经网络模型。

**AND运算**

![image-20201015221951588](https://sanzo.top/img/default/ml/image-20201015221951588.png)

**OR运算**

![image-20201015222018533](https://sanzo.top/img/default/ml/image-20201015222018533.png)

**NOT运算**

![image-20201015222132014](https://sanzo.top/img/default/ml/image-20201015222132014.png)

**XNOR运算**

![image-20201015222208813](https://sanzo.top/img/default/ml/image-20201015222208813.png)

### 9.3 多元分类

通过构建神经网络，每种输出就对应一个分类器。

![image-20201015222645344](https://sanzo.top/img/default/ml/image-20201015222645344.png)

![image-20201015222753618](https://sanzo.top/img/default/ml/image-20201015222753618.png)

### 9.4 代价函数

![image-20201029103442247](https://sanzo.top/img/default/ml/image-20201029103442247.png)

K表示输出层的单元数目，L为神经网络的层数，S(i)表示每一层的神经元数量  

![image-20201029103800511](https://sanzo.top/img/default/ml/image-20201029103800511.png)

### 9.5 前向传播

![image-20201016153630528](https://sanzo.top/img/default/ml/image-20201016153630528.png)

![image-20230601165940026](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/image-20230601165940026.png)

- 前向传播的向量化实现方法

![image-20230601171139136](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/image-20230601171139136.png)

### 9.6 反向传播

![image-20201016153534741](https://sanzo.top/img/default/ml/image-20201016153534741.png)

- 反向传播

![image-20201016153516048](https://sanzo.top/img/default/ml/image-20201016153516048.png)

- 步骤

1. 前向传播
2. 计算误差
3. 反向传播
4. 累积前边的偏导数项
5. 跳出循环，对△进行梯度下降（使用了正则化） 

![image-20201016153457879](https://sanzo.top/img/default/ml/image-20201016153457879.png)

![image-20201016153704540](https://sanzo.top/img/default/ml/image-20201016153704540.png)

### 9.7 随机初始化

在对神经网络进行训练时，theta的取值要随机取值，如果都赋值为0，就会使得每一层的输出值、误差相同，从而存在大量冗余。

![image-20201109212012819](https://sanzo.top/img/default/ml/image-20201109212012819.png)

### 9.8 梯度检测

在实现反向传播算法时，如何确保梯度计算正确呢？

在数学上可以使用拉格朗日中值定理来近似的表示曲线上某一点的导数，梯度检测正是使用的这种思想。

![image-20201106223535108](https://sanzo.top/img/default/ml/image-20201106223535108.png)

梯度检测的使用，可以对每个参数单独进行验证。

![image-20201106223800987](https://sanzo.top/img/default/ml/image-20201106223800987.png)

![image-20201106223848817](https://sanzo.top/img/default/ml/image-20201106223848817.png)

![image-20201109210603659](https://sanzo.top/img/default/ml/image-20201109210603659.png)

假设通过中值定理得到的梯度为approx_grad，经过反向传播得到的梯度为grad，如果满足以下等式，则说明反向传播得到的梯度精度还行。
$$
diff = \frac{||approx\_grad-grad||}{||approx\_grad+grad|||} < 10e^{-9}
$$
梯度计算正确的情况下，当算法进行学习的时候要关闭梯度检测，因为它非常耗时。

![image-20201106223929987](https://sanzo.top/img/default/ml/image-20201106223929987.png)

## 10. 模型评估

### 10.1 训练、测试集

将数据集分为训练集和测试集，训练集得到参数θ，然后使用测试集的数据对参数θ进行评估，即计算误差。

![image-20201017130647268](https://sanzo.top/img/default/ml/image-20201017130647268.png)

线性回归问题的评估：

![image-20201017130713921](https://sanzo.top/img/default/ml/image-20201017130713921.png)

逻辑回归问题的评估：

![image-20201017130740110](https://sanzo.top/img/default/ml/image-20201017130740110.png)

### 10.2 训练、验证、测试集

首先用训练集得到一个最优的参数θ，然后用测试集进行评估误差。通过这样的方式可以在众多模型中选择一个理想的模型。

但是这样做并不能评估模型的**泛化能力**，通过测试集评估选择的模型，可能刚好适合测试集的数据，并不能说明它对其他数据的预测能力，这时就引入了验证集。

![image-20201017131449719](https://sanzo.top/img/default/ml/image-20201017131449719.png)

将数据集分为：训练集、验证集、测试集。

![image-20201017131519699](https://sanzo.top/img/default/ml/image-20201017131519699.png)

对于每个集合都可以计算相应的误差。

![image-20201017131609332](https://sanzo.top/img/default/ml/image-20201017131609332.png)

这样在选择模型的时候，可以先使用测试集得到每个模型的θ\thetaθ，然后使用验证集评估得到误差最小的模型，最后使用测试集评估他的泛化能力。

![image-20201017131812807](https://sanzo.top/img/default/ml/image-20201017131812807.png)

### 10.3 偏差、方差

当多项式次数增大时，训练集的误差慢慢减小，因为多项式次数越高，图像拟合的就越准确。但是验证集不同，它的趋势是先减少后增大，这分别对应着欠拟合和过拟合。

![image-20201017131855857](https://sanzo.top/img/default/ml/image-20201017131855857.png)

那么可以根据误差的不同表现来区分偏差和方差。

高偏差：训练误差和验证误差都很大。

高方差：训练误差小，验证误差大。

![image-20201017132221852](https://sanzo.top/img/default/ml/image-20201017132221852.png)

### 10.4 正则化

通过引入λ来平衡多形式的权重。

当λ太大，参数θ≈0，模型近似直线，即欠拟合。当λ太小，就会出现过拟合。

![image-20230609134355239](https://raw.githubusercontent.com/helloworldzsq/blog-img/main/image-20230609134355239.png)

![image-20201017140936964](https://sanzo.top/img/default/ml/image-20201017140936964.png)

### 10.5 学习曲线

随着数据量的增加，$J_{train}(\theta)$的误差慢慢增大，因为数据越少，模型越容易拟合；$J_{cv}(\theta)$慢慢减少，因为数据越多，模型越精准，所以误差减小。

![image-20201017142111611](https://sanzo.top/img/default/ml/image-20201017142111611.png)

高偏差的模型的学习曲线：

因为参数很少，数据很多，所以随着数据的增多高偏差的模型的$J_{train}(\theta)$和$J_{cv}(\theta)$很接近。这时选择增加数据就不是很好的选择了，可以尝试增加数据的特征。

![image-20201017142532611](https://sanzo.top/img/default/ml/image-20201017142532611.png)

高方差的模型的学习曲线：

高方差的特点是训练误差和验证误差之间有很大的差距，这时可以选择增加数据，随着图像右移可以看出训练误差和验证误差会慢慢接近。

![image-20201017142556080](https://sanzo.top/img/default/ml/image-20201017142556080.png)

- 如何进行抉择

![image-20201017143417016](https://sanzo.top/img/default/ml/image-20201017143417016.png)

![image-20201017143728141](https://sanzo.top/img/default/ml/image-20201017143728141.png)

### 10.6 查准率、召回率

例如对癌症的预测，相对于样本数据真实得癌症的人非常少，大概只有0.5%的概率，这样的问题称为偏斜类，一个类中的样本数比另一个类多得多。

对于偏斜类的问题，如何评估模型的精准度呢？可能一个只输出y=1的函数都比你的模型准确。

这里引入了查准率和召回率，对于稀有的样本有：

![image-20201019123538038](https://sanzo.top/img/default/ml/image-20201019123538038.png)

通常如果阈值设置的比较高，那么对应的查准率高、召回率低；相反如果阈值设置的低，那么查准率低、召回率高。

![image-20201019123701192](https://sanzo.top/img/default/ml/image-20201019123701192.png)

### 10.7 F1 score

如何比较权衡不同的算法呢？

这里使用的F1score，即调和平均数（倒数的平均数）来衡量。

F1score会比较照顾数值小的一方，如果PR都为0，F1score=0；如果PR都为1，F1score=1。

![image-20201019123950436](https://sanzo.top/img/default/ml/image-20201019123950436.png)

![image-20201019124139908](https://sanzo.top/img/default/ml/image-20201019124139908.png)